{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6747216-ac95-4e44-a580-5859f62ad678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import itertools\n",
    "from statistics import mean\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import traceback\n",
    "import logging\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f44164e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIFt(object):\n",
    "    def __init__(self, pickled_sift):\n",
    "        \n",
    "        if pickled_sift is None:\n",
    "            print(\"ohno\")\n",
    "            return None\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize an SIFt class.\n",
    "        Parameters:\n",
    "            fn_pro_PDB - PDB file name of the protein\n",
    "            fn_lig_PDB - PDB file name of the ligand\n",
    "            fn_pro_MOL2 - MOL2 file name of the protein\n",
    "            fn_lig_MOL2 - MOL2 file name of the ligand\n",
    "            ID - ID of the complex\n",
    "            addH - whether to add hydrogen atoms when reading in the structure file\n",
    "            sant - whether to sanitize the molecule when reading in the structure file\n",
    "            int_cutoff - distance threshold for identifying protein-ligand interacting atoms \n",
    "        \"\"\"\n",
    "        self.ID = pickled_sift.ID \n",
    "       # print('Constructing an SIFt object for %s.........\\n' % self.ID)\n",
    "        # read in pdb coordinates and topology\n",
    "        self.lig = pickled_sift.lig\n",
    "        self.pro = pickled_sift.pro\n",
    "\n",
    " \n",
    "       \n",
    "        # parse protein pdb file for identifying sidechain/mainchain atoms\n",
    "#        parser = PDBParser()\n",
    "#        self.structure = parser.get_structure(self.ID, fn_pro_PDB)\n",
    "#        self.chid = self.pro[1].GetAtomWithIdx(0).GetPDBResidueInfo().GetChainId()\n",
    "        # identify interacting area\n",
    "        self.contact_bins = pickled_sift.contact_bins\n",
    "        self.pd = pickled_sift.pd\n",
    "        self.cont = pickled_sift.cont\n",
    "        self.contacts = pickled_sift.contacts\n",
    "\n",
    "        self.protein_env = pickled_sift.protein_env\n",
    "        self.ligand_env = pickled_sift.ligand_env\n",
    "        \n",
    "    #IMCP\n",
    "    def get_quasi_fragmental_desp_ext1(self, bins = None):\n",
    "\n",
    "  \n",
    "        \"\"\"\n",
    "        Compute extended quasi fragmental descriptors.\n",
    "        Parameters:\n",
    "            bins - distance bins for extracting qf descriptors\n",
    "        \"\"\"\n",
    "        if bins is not None:\n",
    "            self.contact_bins = bins\n",
    "        else:\n",
    "            self.contact_bins = [(0, 12)]\n",
    "\n",
    "        # list atom types ----------------------------------------------------\n",
    "        pro_pool = [6, 7, 8, 16]\n",
    "        lig_pool = [6, 7, 8, 9, 15, 16, 17, 35, 53]\n",
    "        # get list of atom-pair types ----------------------------------------\n",
    "        quasi_type = list(itertools.product(pro_pool, lig_pool))        \n",
    "        quasi_feat = []\n",
    "\n",
    "        for cbin in self.contact_bins:\n",
    "            occur = {}\n",
    "            for tp in quasi_type:\n",
    "                occur[tp] = [0, []]\n",
    "            # check the contacts one by one ----------------------------------\n",
    "            contacts = np.nonzero((self.pd >= cbin[0]) & (self.pd < cbin[1]))\n",
    "            conts = [(int(i), int(j)) for (i, j) in zip(contacts[0], contacts[1])]\n",
    "            distances = [self.pd[i, j] for (i, j) in conts]\n",
    "            for ind in range(len(conts)):\n",
    "                cont = conts[ind]\n",
    "                cur_dist = distances[ind]\n",
    "                atm1 = self.pro[1].GetAtomWithIdx(cont[0])\n",
    "                atm2 = self.lig[1].GetAtomWithIdx(cont[1])                \n",
    "                atm1_an = atm1.GetAtomicNum()\n",
    "                atm2_an = atm2.GetAtomicNum()\n",
    "                \n",
    "                tmp = (atm1_an, atm2_an)\n",
    "                if tmp in quasi_type:\n",
    "                    occur[tmp][0] += 1\n",
    "                    occur[tmp][1] += [cur_dist]\n",
    "            \n",
    "            for tp in quasi_type:\n",
    "                if occur[tp][0] == 0:\n",
    "                    quasi_feat += [0, 0]\n",
    "                else:\n",
    "                    quasi_feat += [occur[tp][0], mean(occur[tp][1])]\n",
    "        return quasi_feat  \n",
    "\n",
    "    #IMC\n",
    "\n",
    "    def get_quasi_fragmental_desp_ext2(self, bins = None):\n",
    "        \"\"\"\n",
    "        Compute extended quasi fragmental descriptors.\n",
    "        Parameters:\n",
    "            bins - distance bins for extracting qf descriptors\n",
    "        \"\"\"\n",
    "        if bins is not None:\n",
    "            self.contact_bins = bins\n",
    "        else:\n",
    "            self.contact_bins = [(0, 12)]\n",
    "\n",
    "        # list atom types ----------------------------------------------------\n",
    "        pro_pool = [6, 7, 8, 16]\n",
    "        lig_pool = [6, 7, 8, 9, 15, 16, 17, 35, 53]\n",
    "        # get list of atom-pair types ----------------------------------------\n",
    "        quasi_type = list(itertools.product(pro_pool, lig_pool))        \n",
    "        quasi_feat = []\n",
    "\n",
    "        for cbin in self.contact_bins:\n",
    "            occur = {}\n",
    "            for tp in quasi_type:\n",
    "                occur[tp] = [0, []]\n",
    "            # check the contacts one by one ----------------------------------\n",
    "            contacts = np.nonzero((self.pd >= cbin[0]) & (self.pd < cbin[1]))\n",
    "            conts = [(int(i), int(j)) for (i, j) in zip(contacts[0], contacts[1])]\n",
    "            distances = [self.pd[i, j] for (i, j) in conts]\n",
    "            for ind in range(len(conts)):\n",
    "                cont = conts[ind]\n",
    "                cur_dist = distances[ind]\n",
    "                atm1 = self.pro[1].GetAtomWithIdx(cont[0])\n",
    "                atm2 = self.lig[1].GetAtomWithIdx(cont[1])                \n",
    "                atm1_an = atm1.GetAtomicNum()\n",
    "                atm2_an = atm2.GetAtomicNum()\n",
    "                \n",
    "                tmp = (atm1_an, atm2_an)\n",
    "                if tmp in quasi_type:\n",
    "                    occur[tmp][0] += 1\n",
    "                    occur[tmp][1] += [cur_dist]\n",
    "            \n",
    "            for tp in quasi_type:\n",
    "                if occur[tp][0] == 0:\n",
    "                    quasi_feat += [0]\n",
    "                else:\n",
    "                    quasi_feat += [occur[tp][0]]\n",
    "        return quasi_feat  \n",
    "\n",
    "\n",
    "    # protien env weighted\n",
    "    def get_quasi_fragmental_desp_ext3(self, bins = None):\n",
    "            \"\"\"\n",
    "            Compute extended quasi fragmental descriptors.\n",
    "            Parameters:\n",
    "                bins - distance bins for extracting qf descriptors\n",
    "            \"\"\"\n",
    "            if bins is not None:\n",
    "                self.contact_bins = bins\n",
    "            else:\n",
    "                self.contact_bins = [(0, 12)]\n",
    "\n",
    "            # list atom types ----------------------------------------------------\n",
    "            pro_pool = [6, 7, 8, 16]\n",
    "            lig_pool = [6, 7, 8, 9, 15, 16, 17, 35, 53]\n",
    "            # get list of atom-pair types ----------------------------------------\n",
    "            quasi_type = list(itertools.product(pro_pool, lig_pool))        \n",
    "            quasi_feat = []\n",
    "            debug = defaultdict(int)\n",
    "            for cbin in self.contact_bins:\n",
    "                occur = {}\n",
    "                for tp in quasi_type:\n",
    "                    occur[tp] = [0, [],[]]\n",
    "                # check the contacts one by one ----------------------------------\n",
    "                contacts = np.nonzero((self.pd >= cbin[0]) & (self.pd < cbin[1]))\n",
    "                conts = [(int(i), int(j)) for (i, j) in zip(contacts[0], contacts[1])]\n",
    "                distances = [self.pd[i, j] for (i, j) in conts]\n",
    "                for ind in range(len(conts)):\n",
    "                    cont = conts[ind]\n",
    "                    cur_dist = distances[ind]\n",
    "                    atm1 = self.pro[1].GetAtomWithIdx(cont[0])\n",
    "                    atm2 = self.lig[1].GetAtomWithIdx(cont[1])                \n",
    "                    atm1_an = atm1.GetAtomicNum()\n",
    "                    atm2_an = atm2.GetAtomicNum()\n",
    "                    \n",
    "                    tmp = (atm1_an, atm2_an)\n",
    "                    if tmp in quasi_type:\n",
    "                        occur[tmp][0] += 1\n",
    "                        env1 = self.protein_env.get(cont[0], 1)\n",
    "                        env2 = self.ligand_env.get(cont[1], 1)\n",
    "                        debug[(env1,env2)] += 1\n",
    "                        occur[tmp][1] += [ ( 1/env1 ) * cur_dist]\n",
    "                        occur[tmp][2] += [( 1/env1 ) ]\n",
    "                \n",
    "                for tp in quasi_type:\n",
    "                    if occur[tp][0] == 0:\n",
    "                        quasi_feat += [0, 0]\n",
    "                    else:\n",
    "                        quasi_feat += [occur[tp][0], sum(occur[tp][1])/ sum(occur[tp][2])]\n",
    "\n",
    "            return quasi_feat  \n",
    "\n",
    "    # IMCEP\n",
    "    def get_quasi_fragmental_desp_ext4(self, bins = None):\n",
    "            \"\"\"\n",
    "            Compute extended quasi fragmental descriptors.\n",
    "            Parameters:\n",
    "                bins - distance bins for extracting qf descriptors\n",
    "            \"\"\"\n",
    "            if bins is not None:\n",
    "                self.contact_bins = bins\n",
    "            else:\n",
    "                self.contact_bins = [(0, 12)]\n",
    "\n",
    "            # list atom types ----------------------------------------------------\n",
    "            pro_pool = [6, 7, 8, 16]\n",
    "            lig_pool = [6, 7, 8, 9, 15, 16, 17, 35, 53]\n",
    "            # get list of atom-pair types ----------------------------------------\n",
    "            quasi_type = list(itertools.product(pro_pool, lig_pool))        \n",
    "            quasi_feat = []\n",
    "            debug = defaultdict(int)\n",
    "            for cbin in self.contact_bins:\n",
    "                occur = {}\n",
    "                for tp in quasi_type:\n",
    "                    occur[tp] = [0, [], [], []]\n",
    "                # check the contacts one by one ----------------------------------\n",
    "                contacts = np.nonzero((self.pd >= cbin[0]) & (self.pd < cbin[1]))\n",
    "                conts = [(int(i), int(j)) for (i, j) in zip(contacts[0], contacts[1])]\n",
    "                distances = [self.pd[i, j] for (i, j) in conts]\n",
    "                for ind in range(len(conts)):\n",
    "                    cont = conts[ind]\n",
    "                    cur_dist = distances[ind]\n",
    "                    atm1 = self.pro[1].GetAtomWithIdx(cont[0])\n",
    "                    atm2 = self.lig[1].GetAtomWithIdx(cont[1])                \n",
    "                    atm1_an = atm1.GetAtomicNum()\n",
    "                    atm2_an = atm2.GetAtomicNum()\n",
    "                    \n",
    "                    tmp = (atm1_an, atm2_an)\n",
    "                    if tmp in quasi_type:\n",
    "                        occur[tmp][0] += 1\n",
    "                        env1 = self.protein_env.get(cont[0], 1)\n",
    "                        env2 = self.ligand_env.get(cont[1], 1)\n",
    "                        debug[(env1,env2)] += 1\n",
    "                        occur[tmp][1] += [cur_dist]\n",
    "                        occur[tmp][2] += [ 1/env1 ]\n",
    "                        occur[tmp][3] += [ env2 ]\n",
    "                \n",
    "                for tp in quasi_type:\n",
    "                    if occur[tp][0] == 0:\n",
    "                        quasi_feat += [0,0,0]\n",
    "                    else:\n",
    "                        quasi_feat += [occur[tp][0], mean(occur[tp][1]),mean(occur[tp][2])]\n",
    "\n",
    "            return quasi_feat  \n",
    "\n",
    "    #IMCPiEB\n",
    "    def get_quasi_fragmental_desp_ext5(self, bins = None):\n",
    "            \"\"\"\n",
    "            Compute extended quasi fragmental descriptors.\n",
    "            Parameters:\n",
    "                bins - distance bins for extracting qf descriptors\n",
    "            \"\"\"\n",
    "            if bins is not None:\n",
    "                self.contact_bins = bins\n",
    "            else:\n",
    "                self.contact_bins = [(0, 12)]\n",
    "\n",
    "            # list atom types ----------------------------------------------------\n",
    "            pro_pool = [6, 7, 8, 16]\n",
    "            lig_pool = [6, 7, 8, 9, 15, 16, 17, 35, 53]\n",
    "            # get list of atom-pair types ----------------------------------------\n",
    "            quasi_type = list(itertools.product(pro_pool, lig_pool))        \n",
    "            quasi_feat = []\n",
    "            debug = defaultdict(int)\n",
    "            for cbin in self.contact_bins:\n",
    "                occur = {}\n",
    "                for tp in quasi_type:\n",
    "                    occur[tp] = [0, [], [], []]\n",
    "                # check the contacts one by one ----------------------------------\n",
    "                contacts = np.nonzero((self.pd >= cbin[0]) & (self.pd < cbin[1]))\n",
    "                conts = [(int(i), int(j)) for (i, j) in zip(contacts[0], contacts[1])]\n",
    "                distances = [self.pd[i, j] for (i, j) in conts]\n",
    "                for ind in range(len(conts)):\n",
    "                    cont = conts[ind]\n",
    "                    cur_dist = distances[ind]\n",
    "                    atm1 = self.pro[1].GetAtomWithIdx(cont[0])\n",
    "                    atm2 = self.lig[1].GetAtomWithIdx(cont[1])                \n",
    "                    atm1_an = atm1.GetAtomicNum()\n",
    "                    atm2_an = atm2.GetAtomicNum()\n",
    "                    \n",
    "                    tmp = (atm1_an, atm2_an)\n",
    "                    if tmp in quasi_type:\n",
    "                        occur[tmp][0] += 1\n",
    "                        env1 = self.protein_env.get(cont[0], 1)\n",
    "                        env2 = self.ligand_env.get(cont[1], 1)\n",
    "                        debug[(env1,env2)] += 1\n",
    "                        occur[tmp][1] += [cur_dist]\n",
    "                        occur[tmp][2] += [ 1/env1 ]\n",
    "                        occur[tmp][3] += [ env2 ]\n",
    "                \n",
    "                for tp in quasi_type:\n",
    "                    if occur[tp][0] == 0:\n",
    "                        quasi_feat += [0,0,0,0]\n",
    "                    else:\n",
    "                        mean_dis = mean(occur[tp][2])\n",
    "                        dists = occur[tp][1]\n",
    "                        vals = occur[tp][2]\n",
    "\n",
    "                        count1 = dist1 = 0\n",
    "                        count2 = dist2 = 0\n",
    "\n",
    "                        for d, v in zip(dists, vals):\n",
    "                            if v <= mean_dis:\n",
    "                                count1 += 1\n",
    "                                dist1 += d\n",
    "                            else:\n",
    "                                count2 += 1\n",
    "                                dist2 += d\n",
    "\n",
    "                        if count1 == 0:\n",
    "                            if count2 == 0:\n",
    "                                quasi_feat += [0, 0, 0, 0]\n",
    "                            else:\n",
    "                                quasi_feat += [0, 0, count2, dist2 / count2]\n",
    "                        else:\n",
    "                            if count2 == 0:\n",
    "                                quasi_feat += [count1, dist1 / count1, 0, 0]\n",
    "                            else:\n",
    "                                quasi_feat += [count1, dist1 / count1, count2, dist2 / count2]\n",
    "\n",
    "\n",
    "            return quasi_feat  \n",
    "    #MIX\n",
    "    def get_quasi_fragmental_desp_ext6(self, bins = None):\n",
    "            \"\"\"\n",
    "            Compute extended quasi fragmental descriptors.\n",
    "            Parameters:\n",
    "                bins - distance bins for extracting qf descriptors\n",
    "            \"\"\"\n",
    "            if bins is not None:\n",
    "                self.contact_bins = bins\n",
    "            else:\n",
    "                self.contact_bins = [(0, 12)]\n",
    "\n",
    "            # list atom types ----------------------------------------------------\n",
    "            pro_pool = [6, 7, 8, 16]\n",
    "            lig_pool = [6, 7, 8, 9, 15, 16, 17, 35, 53]\n",
    "            # get list of atom-pair types ----------------------------------------\n",
    "            quasi_type = list(itertools.product(pro_pool, lig_pool))        \n",
    "            quasi_feat = []\n",
    "            debug = defaultdict(int)\n",
    "            for cbin in self.contact_bins:\n",
    "                occur = {}\n",
    "                for tp in quasi_type:\n",
    "                    occur[tp] = [0, [], [], []]\n",
    "                # check the contacts one by one ----------------------------------\n",
    "                contacts = np.nonzero((self.pd >= cbin[0]) & (self.pd < cbin[1]))\n",
    "                conts = [(int(i), int(j)) for (i, j) in zip(contacts[0], contacts[1])]\n",
    "                distances = [self.pd[i, j] for (i, j) in conts]\n",
    "                for ind in range(len(conts)):\n",
    "                    cont = conts[ind]\n",
    "                    cur_dist = distances[ind]\n",
    "                    atm1 = self.pro[1].GetAtomWithIdx(cont[0])\n",
    "                    atm2 = self.lig[1].GetAtomWithIdx(cont[1])                \n",
    "                    atm1_an = atm1.GetAtomicNum()\n",
    "                    atm2_an = atm2.GetAtomicNum()\n",
    "                    \n",
    "                    tmp = (atm1_an, atm2_an)\n",
    "                    if tmp in quasi_type:\n",
    "                        occur[tmp][0] += 1\n",
    "                        env1 = self.protein_env.get(cont[0], 1)\n",
    "                        env2 = self.ligand_env.get(cont[1], 1)\n",
    "                        debug[(env1,env2)] += 1\n",
    "                        occur[tmp][1] += [cur_dist]\n",
    "                        occur[tmp][2] += [ 1/env1 ]\n",
    "                        occur[tmp][3] += [ env2 ]\n",
    "                \n",
    "                for tp in quasi_type:\n",
    "                    if occur[tp][0] == 0:\n",
    "                        quasi_feat += [0,0,0,0]\n",
    "                    else:\n",
    "                        quasi_feat += [occur[tp][0], mean(occur[tp][1]),mean(occur[tp][2]),mean(occur[tp][3])]\n",
    "\n",
    "            return quasi_feat  \n",
    "                \n",
    "\n",
    "\n",
    "            return quasi_feat  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1e50d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _featurize_complex(pdbinfo, log_message, para,model):\n",
    "    \"\"\"Featurizes a complex.\n",
    "    First initializes an SIFt object, and then calculates the intlst.\n",
    "    \"\"\"   \n",
    "    logging.info(log_message)\n",
    "    protein_path = pdbinfo['fn_pro_PDB']\n",
    "    pkl_path = protein_path.replace('_protein.pdb', '_sift.pkl')\n",
    "\n",
    "# Check if the corresponding pickle file exists\n",
    "    if os.path.isfile(pkl_path):\n",
    "        pass\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # try:\n",
    "    try :\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            pickled_sift_obj = pickle.load(f)\n",
    "    except:\n",
    "        return None\n",
    "    sift = SIFt(pickled_sift_obj)\n",
    "    if sift is None:\n",
    "        print(\"error in retriving object\")\n",
    "        return None\n",
    "    desp = None\n",
    "    if model == 1:\n",
    "        desp= sift.get_quasi_fragmental_desp_ext1(bins = para['bins'])\n",
    "    if model == 2:\n",
    "        desp= sift.get_quasi_fragmental_desp_ext2(bins = para['bins'])\n",
    "    if model == 3:\n",
    "        desp= sift.get_quasi_fragmental_desp_ext3(bins = para['bins'])\n",
    "    if model == 4:\n",
    "        desp= sift.get_quasi_fragmental_desp_ext4(bins = para['bins'])\n",
    "    if model == 5:\n",
    "        desp= sift.get_quasi_fragmental_desp_ext5(bins = para['bins'])\n",
    "    if model == 6:\n",
    "        desp= sift.get_quasi_fragmental_desp_ext6(bins = para['bins'])\n",
    "\n",
    "    if (desp is None):\n",
    "        print(\"wtf\")\n",
    "    return desp\n",
    "#     except:\n",
    "#         print(\"Error during featurization:\")\n",
    "# #        traceback.print_exc()\n",
    "#         return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0636b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_complexes(ligand_pdbfiles, \n",
    "                        protein_pdbfiles,\n",
    "                        ligand_mol2files, \n",
    "                        protein_mol2files,\n",
    "                        pdbids,model,\n",
    "                        para = {'addH': True, 'sant': True,\n",
    "                                'cutoff': 4.5, 'includeH': False, 'ifp_type': 'sift', 'count': 1,\n",
    "                                'intlst': ['contact', 'backbone', 'sidechain', 'polar', 'nonpolar', 'donor', 'acceptor'],\n",
    "                                'inttype': 'CH'}\n",
    "#                        para = {'addH': True, 'sant': True,\n",
    "#                                'cutoff': 4.5, 'ifp_type': 'rfscore',\n",
    "#                                'bins': None, # use default contact bins\n",
    "#                                'solo': 0, 'lst': []}\n",
    "#                        para = {'addH': True, 'sant': False,\n",
    "#                                'cutoff': 4.5, 'ifp_type': 'splif',\n",
    "#                                'ecfp_radius': [1, 1],\n",
    "#                                'base_prop': ['AtomicNumber', 'TotalConnections', 'HCount', 'HeavyNeighborCount', 'FormalCharge'],\n",
    "#                                'folding_para': {'power': np.arange(6, 8, 1), 'counts': 1}}\n",
    "                        ):\n",
    "    \"\"\"Obtains SIFts of a group of complexes.\n",
    "    Parameters:\n",
    "        ligand_pdbfiles - a list of ligand pdb files\n",
    "        protein_pdbfiles - a list of protein pdb files\n",
    "        ligand_mol2files - a list of ligand mol2 files\n",
    "        protein_mol2files - a list of protein mol2 files\n",
    "        pdbids - a list of pdb ids for the complexes under processing\n",
    "        para - parameters for constructing SIFts\n",
    "    Returns a list of the SIFts and the indices of failed complexes.\n",
    "    \"\"\"\n",
    "    pool = multiprocessing.Pool(processes =8)\n",
    "    results = []\n",
    "    feat = []\n",
    "    failures = []\n",
    "    info = zip(ligand_pdbfiles, protein_pdbfiles, ligand_mol2files, protein_mol2files, pdbids)\n",
    "    for i, (lig_pdbfile, pro_pdbfile, lig_mol2file, pro_mol2file, pdbid) in enumerate(info):\n",
    "        #print(i)\n",
    "        log_message = \"Featurizing %d / %d complex...\" % (i, len(pdbids))\n",
    "        pdbinfo = {'fn_pro_PDB': pro_pdbfile, 'fn_lig_PDB': lig_pdbfile, \n",
    "                   'fn_pro_MOL2': pro_mol2file, 'fn_lig_MOL2': lig_mol2file, 'pdbid': pdbid}\n",
    "        results.append(pool.apply_async(_featurize_complex, (pdbinfo, log_message, para,model)))      \n",
    "    pool.close()  \n",
    "    pool.join()\n",
    "    for ind, result in enumerate(results):\n",
    "        new_sift = result.get()\n",
    "        if new_sift is None:\n",
    "            failures.append(ind)\n",
    "        else:\n",
    "            feat.append(new_sift)\n",
    "    \n",
    "    return feat, failures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1953f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata_fromPDBbind_v2020(data_dir,modele,\n",
    "                               subset = \"refined\",\n",
    "                               select_target = ['HIV-1 PROTEASE'],\n",
    "                               randsplit_ratio = [0.5, 0.25, 0.25],\n",
    "                               para = {'addH': True, 'sant': True,\n",
    "                                       'cutoff': 4.5, 'ifp_type': 'rfscore',\n",
    "                                       'bins': None, 'lst': [], # use default contact bins\n",
    "                                       'solo': 0},\n",
    "                               rand_seed = 123):\n",
    "    dt_dir = data_dir\n",
    "    os.chdir(dt_dir)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------------------------\n",
    "    # get folder containing the structural data and the index dataframe containing the pdbs/affinities\n",
    "    # -----------------------------------------------------------------------------------------------------------------------\n",
    "    data_folder_dict = {'refined': 'data/'}\n",
    "    data_index_dict = {'refined': 'indexes/rs_index.csv'}\n",
    "    for s in ['casf2016']:\n",
    "        data_folder_dict[s] = 'data/'\n",
    "        data_index_dict[s] = 'indexes/' + s + '_index.csv'\n",
    "\n",
    "    if subset in ['casf2016']:\n",
    "        cur_fd = data_folder_dict[subset]\n",
    "        df_index = pd.read_csv(data_index_dict[subset])\n",
    "    else:\n",
    "        cur_fd = data_folder_dict['refined']\n",
    "        df_index_all = pd.read_csv(data_index_dict['refined'])\n",
    "        df_index = df_index_all\n",
    "        \n",
    "    # further filter the dataframe according to select_target\n",
    "    pdbs_selected = df_index['id'].tolist()\n",
    "    labels_selected = df_index['affinity'].astype(float).tolist()\n",
    "            \n",
    "    # obtain the pdb and mol2 filename lists for the complexes -----------------------------------------------------------------------\n",
    "    print('Generate file names................................................................')\n",
    "    protein_pdbfiles = []\n",
    "    ligand_pdbfiles = []\n",
    "    protein_mol2files = []\n",
    "    ligand_mol2files = []\n",
    "    for pdb in pdbs_selected:\n",
    "        protein_pdbfiles += [os.path.join(dt_dir, cur_fd, pdb, \"%s_protein.pdb\" % pdb)]\n",
    "        protein_mol2files += [os.path.join(dt_dir, cur_fd, pdb, \"%s_protein.mol2\" % pdb)]\n",
    "        ligand_pdbfiles += [os.path.join(dt_dir, cur_fd, pdb, \"%s_ligand.pdb\" % pdb)]\n",
    "        ligand_mol2files += [os.path.join(dt_dir, cur_fd, pdb, \"%s_ligand.mol2\" % pdb)]  \n",
    "        \n",
    "    # featurize complexes using SIFts and split them into train, validation and test sets --------------------------------------------\n",
    "    print('Begin to featurize dataset....................................................................')\n",
    "    feat_t1 = time.time()\n",
    "    feat, flrs = featurize_complexes(ligand_pdbfiles = ligand_pdbfiles, \n",
    "                                     protein_pdbfiles = protein_pdbfiles,\n",
    "                                     ligand_mol2files = ligand_mol2files, \n",
    "                                     protein_mol2files = protein_mol2files,\n",
    "                                     pdbids = pdbs_selected,model = modele,\n",
    "                                     para = para)\n",
    "    # Delete labels and ids for failing elements\n",
    "    labels_lft = np.delete(labels_selected, flrs)\n",
    "    return [np.array(feat),np.array(labels_lft)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08e0f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_models(X, y, n_splits=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform K-Fold CV for three regression models and return CV metrics.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest9': RandomForestRegressor(random_state=random_state,n_estimators = 650),\n",
    "        'GradientBoosting9': GradientBoostingRegressor(random_state=random_state,n_estimators=700),\n",
    "        \"svr_poly1\" : SVR(kernel='poly', degree=6, C=1.0, epsilon=0.1, coef0=1),\n",
    "        \"XGB2\" : XGBRegressor(objective='reg:squarederror', learning_rate=0.1, max_depth=10),\n",
    "        \"Linear\" : LinearRegression()\n",
    "        \n",
    "    }\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    cv_results = {name: {'pearson': [], 'rmse': []} for name in models}\n",
    "    # return cv_results, models\n",
    "    for name, model in models.items():\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_val)\n",
    "            # Metrics\n",
    "            r, _ = pearsonr(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            cv_results[name]['pearson'].append(r)\n",
    "            cv_results[name]['rmse'].append(rmse)\n",
    "\n",
    "        # Aggregate\n",
    "        pearson_vals = cv_results[name]['pearson']\n",
    "        rmse_vals    = cv_results[name]['rmse']\n",
    "\n",
    "        pearson_mean = np.mean(pearson_vals)\n",
    "        pearson_std  = np.std(pearson_vals)   # sample std\n",
    "        rmse_mean    = np.mean(rmse_vals)\n",
    "        rmse_std     = np.std(rmse_vals)      # sample std\n",
    "\n",
    "        print(\n",
    "            f\"{name} CV -> \"\n",
    "            f\"Pearson: {pearson_mean:.3f} ± {pearson_std:.3f}, \"\n",
    "            f\"RMSE: {rmse_mean:.3f} ± {rmse_std:.3f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "    return cv_results, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3715cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_test(X_train, y_train, X_test, y_test):\n",
    "    # Load training data\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    # 2. Fit PCA on the scaled training data and transform\n",
    "    pca = PCA(n_components=0.95)  # retain 95% of variance\n",
    "    X_pca_train = pca.fit_transform(X_scaled_train)\n",
    "\n",
    "    # 3. Transform test data with the *same* scaler and PCA\n",
    "    X_scaled_test = scaler.transform(X_test)\n",
    "    X_pca_test   = pca.transform(X_scaled_test)\n",
    "    \n",
    "    print(f\"Training samples: {X_pca_train.shape[0]}, features: {X_pca_train.shape[1]}\")\n",
    "\n",
    "    # Cross-validate\n",
    "    cv_results, models = cross_validate_models(X_pca_train, y_train)\n",
    "\n",
    "    # Train final models on all training data and evaluate on test set\n",
    "    print(f\"Test samples: {X_pca_test.shape[0]}\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_pca_train, y_train)\n",
    "        y_pred = model.predict(X_pca_train)\n",
    "\n",
    "\n",
    "        \n",
    "        # f = open('/home/ansh-meshram/Desktop/work/bio_project/pred_data.txt', 'a')\n",
    "        # # Write the variables to the file\n",
    "        # f.write(f\"Name: {name}\\n\")\n",
    "        # f.write(f\"pred 1: {y_pred}\\n\")\n",
    "        # f.write(f\"test 2: {y_test}\\n\")\n",
    "        # # Close the file\n",
    "        # f.close()\n",
    "\n",
    "\n",
    "        r, _ = pearsonr(y_train, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "        print(f\" TRAIN :: {name} Test -> Pearson: {r:.3f}, RMSE: {rmse:.3f}\")\n",
    "\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_pca_train, y_train)\n",
    "        y_pred = model.predict(X_pca_test)\n",
    "\n",
    "\n",
    "        # f = open('/home/ansh-meshram/Desktop/work/bio_project/pred_data.txt', 'a')\n",
    "        # # Write the variables to the file\n",
    "        # f.write(f\"Name: {name}\\n\")\n",
    "        # f.write(f\"pred 1: {y_pred}\\n\")\n",
    "        # f.write(f\"test 2: {y_test}\\n\")\n",
    "        # # Close the file\n",
    "        # f.close()\n",
    "        r, _ = pearsonr(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        print(f\"TEST :: {name} Test -> Pearson: {r:.3f}, RMSE: {rmse:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "961163cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def round_robin_split_and_plot_correlation(X, y, N):\n",
    "    \"\"\"\n",
    "    Splits the feature matrix (X) into N groups in round-robin fashion,\n",
    "    computes the correlation of each feature in the group with the target (y),\n",
    "    and plots a bar chart of correlations for each group.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: 2D numpy array (num_samples, num_features) containing feature values\n",
    "    - y: List or numpy array of target values (num_samples,)\n",
    "    - N: Number of groups to split the features into\n",
    "    \"\"\"\n",
    "    # Step 1: Divide the feature matrix (X) into N groups using round-robin\n",
    "    num_features = X.shape[1]\n",
    "    feature_groups = [[] for _ in range(N)]  # Create empty lists for each group\n",
    "    \n",
    "    # Round-robin assignment of features to groups\n",
    "    for i in range(num_features):\n",
    "        feature_groups[i % N].append(X[:, i])  # Assign each feature to a group\n",
    "    \n",
    "    # Convert each group of features into a matrix (stack columns together)\n",
    "    for i in range(N):\n",
    "        feature_groups[i] = np.column_stack(feature_groups[i])\n",
    "    \n",
    "    # Step 2: Compute correlation and plot for each group\n",
    "    for i, group in enumerate(feature_groups):\n",
    "        # Compute the Pearson correlation between each feature in the group and the target (y)\n",
    "        correlations = [np.corrcoef(group[:, j], y)[0, 1] for j in range(group.shape[1])]\n",
    "        \n",
    "        # Create a bar plot of the correlations\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(range(group.shape[1]), correlations, color='lightblue')\n",
    "        plt.title(f'Group {i + 1} - Correlation of Features with Target')\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Pearson Correlation')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f89ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number => 3\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 23\n",
      "RandomForest9 CV -> Pearson: 0.678 ± 0.027, RMSE: 1.432 ± 0.050\n",
      "GradientBoosting9 CV -> Pearson: 0.639 ± 0.031, RMSE: 1.500 ± 0.042\n",
      "svr_poly1 CV -> Pearson: 0.249 ± 0.184, RMSE: 26.044 ± 44.921\n",
      "XGB2 CV -> Pearson: 0.658 ± 0.025, RMSE: 1.470 ± 0.047\n",
      "Linear CV -> Pearson: 0.573 ± 0.029, RMSE: 1.598 ± 0.044\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.976, RMSE: 0.528\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.889, RMSE: 0.936\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.771, RMSE: 1.242\n",
      " TRAIN :: XGB2 Test -> Pearson: 0.997, RMSE: 0.167\n",
      " TRAIN :: Linear Test -> Pearson: 0.578, RMSE: 1.591\n",
      "TEST :: RandomForest9 Test -> Pearson: 0.714, RMSE: 1.425\n",
      "TEST :: GradientBoosting9 Test -> Pearson: 0.685, RMSE: 1.469\n",
      "TEST :: svr_poly1 Test -> Pearson: 0.486, RMSE: 1.974\n",
      "TEST :: XGB2 Test -> Pearson: 0.708, RMSE: 1.425\n",
      "TEST :: Linear Test -> Pearson: 0.647, RMSE: 1.554\n",
      "Model Number => 4\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 25\n",
      "RandomForest9 CV -> Pearson: 0.673 ± 0.028, RMSE: 1.441 ± 0.049\n",
      "GradientBoosting9 CV -> Pearson: 0.634 ± 0.019, RMSE: 1.510 ± 0.054\n",
      "svr_poly1 CV -> Pearson: 0.329 ± 0.189, RMSE: 5.798 ± 5.564\n",
      "XGB2 CV -> Pearson: 0.653 ± 0.030, RMSE: 1.477 ± 0.049\n",
      "Linear CV -> Pearson: 0.571 ± 0.026, RMSE: 1.600 ± 0.044\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.976, RMSE: 0.530\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.891, RMSE: 0.928\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.780, RMSE: 1.222\n",
      " TRAIN :: XGB2 Test -> Pearson: 0.996, RMSE: 0.194\n",
      " TRAIN :: Linear Test -> Pearson: 0.578, RMSE: 1.591\n",
      "TEST :: RandomForest9 Test -> Pearson: 0.726, RMSE: 1.405\n",
      "TEST :: GradientBoosting9 Test -> Pearson: 0.683, RMSE: 1.472\n",
      "TEST :: svr_poly1 Test -> Pearson: 0.548, RMSE: 1.830\n",
      "TEST :: XGB2 Test -> Pearson: 0.719, RMSE: 1.408\n",
      "TEST :: Linear Test -> Pearson: 0.641, RMSE: 1.561\n",
      "Model Number => 5\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 45\n",
      "RandomForest9 CV -> Pearson: 0.671 ± 0.035, RMSE: 1.448 ± 0.050\n",
      "GradientBoosting9 CV -> Pearson: 0.627 ± 0.026, RMSE: 1.522 ± 0.046\n",
      "svr_poly1 CV -> Pearson: 0.264 ± 0.154, RMSE: 7.583 ± 7.323\n",
      "XGB2 CV -> Pearson: 0.659 ± 0.031, RMSE: 1.466 ± 0.054\n",
      "Linear CV -> Pearson: 0.584 ± 0.028, RMSE: 1.582 ± 0.041\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.978, RMSE: 0.532\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.913, RMSE: 0.848\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.832, RMSE: 1.082\n",
      " TRAIN :: XGB2 Test -> Pearson: 0.999, RMSE: 0.084\n",
      " TRAIN :: Linear Test -> Pearson: 0.593, RMSE: 1.569\n",
      "TEST :: RandomForest9 Test -> Pearson: 0.736, RMSE: 1.389\n",
      "TEST :: GradientBoosting9 Test -> Pearson: 0.681, RMSE: 1.475\n",
      "TEST :: svr_poly1 Test -> Pearson: 0.490, RMSE: 2.281\n",
      "TEST :: XGB2 Test -> Pearson: 0.719, RMSE: 1.401\n",
      "TEST :: Linear Test -> Pearson: 0.642, RMSE: 1.554\n",
      "Model Number => 6\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 27\n",
      "RandomForest9 CV -> Pearson: 0.667 ± 0.028, RMSE: 1.454 ± 0.040\n",
      "GradientBoosting9 CV -> Pearson: 0.631 ± 0.020, RMSE: 1.514 ± 0.040\n",
      "svr_poly1 CV -> Pearson: 0.394 ± 0.164, RMSE: 3.589 ± 2.873\n",
      "XGB2 CV -> Pearson: 0.644 ± 0.023, RMSE: 1.493 ± 0.038\n",
      "Linear CV -> Pearson: 0.569 ± 0.026, RMSE: 1.602 ± 0.044\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.977, RMSE: 0.533\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.894, RMSE: 0.920\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.801, RMSE: 1.167\n",
      " TRAIN :: XGB2 Test -> Pearson: 0.997, RMSE: 0.164\n",
      " TRAIN :: Linear Test -> Pearson: 0.577, RMSE: 1.593\n",
      "TEST :: RandomForest9 Test -> Pearson: 0.723, RMSE: 1.416\n",
      "TEST :: GradientBoosting9 Test -> Pearson: 0.668, RMSE: 1.500\n",
      "TEST :: svr_poly1 Test -> Pearson: 0.623, RMSE: 1.655\n",
      "TEST :: XGB2 Test -> Pearson: 0.712, RMSE: 1.421\n",
      "TEST :: Linear Test -> Pearson: 0.631, RMSE: 1.575\n",
      "Model Number => 3\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 38\n",
      "RandomForest9 CV -> Pearson: 0.692 ± 0.029, RMSE: 1.412 ± 0.058\n",
      "GradientBoosting9 CV -> Pearson: 0.647 ± 0.026, RMSE: 1.489 ± 0.055\n",
      "svr_poly1 CV -> Pearson: 0.269 ± 0.178, RMSE: 10.783 ± 17.278\n",
      "XGB2 CV -> Pearson: 0.674 ± 0.033, RMSE: 1.439 ± 0.063\n",
      "Linear CV -> Pearson: 0.586 ± 0.032, RMSE: 1.579 ± 0.043\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.978, RMSE: 0.519\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.907, RMSE: 0.869\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.810, RMSE: 1.144\n",
      " TRAIN :: XGB2 Test -> Pearson: 0.999, RMSE: 0.104\n",
      " TRAIN :: Linear Test -> Pearson: 0.595, RMSE: 1.567\n",
      "TEST :: RandomForest9 Test -> Pearson: 0.745, RMSE: 1.375\n",
      "TEST :: GradientBoosting9 Test -> Pearson: 0.693, RMSE: 1.451\n",
      "TEST :: svr_poly1 Test -> Pearson: 0.346, RMSE: 2.807\n",
      "TEST :: XGB2 Test -> Pearson: 0.733, RMSE: 1.374\n",
      "TEST :: Linear Test -> Pearson: 0.643, RMSE: 1.554\n",
      "Model Number => 4\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 41\n",
      "RandomForest9 CV -> Pearson: 0.693 ± 0.026, RMSE: 1.410 ± 0.046\n",
      "GradientBoosting9 CV -> Pearson: 0.648 ± 0.021, RMSE: 1.485 ± 0.040\n",
      "svr_poly1 CV -> Pearson: 0.339 ± 0.167, RMSE: 4.569 ± 3.951\n",
      "XGB2 CV -> Pearson: 0.676 ± 0.020, RMSE: 1.435 ± 0.037\n",
      "Linear CV -> Pearson: 0.583 ± 0.030, RMSE: 1.584 ± 0.046\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.978, RMSE: 0.520\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.912, RMSE: 0.848\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.818, RMSE: 1.122\n",
      " TRAIN :: XGB2 Test -> Pearson: 0.999, RMSE: 0.101\n",
      " TRAIN :: Linear Test -> Pearson: 0.592, RMSE: 1.572\n",
      "TEST :: RandomForest9 Test -> Pearson: 0.725, RMSE: 1.406\n",
      "TEST :: GradientBoosting9 Test -> Pearson: 0.685, RMSE: 1.468\n",
      "TEST :: svr_poly1 Test -> Pearson: 0.485, RMSE: 2.104\n",
      "TEST :: XGB2 Test -> Pearson: 0.722, RMSE: 1.399\n",
      "TEST :: Linear Test -> Pearson: 0.655, RMSE: 1.535\n",
      "Model Number => 5\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 91\n",
      "RandomForest9 CV -> Pearson: 0.668 ± 0.029, RMSE: 1.459 ± 0.051\n",
      "GradientBoosting9 CV -> Pearson: 0.634 ± 0.031, RMSE: 1.509 ± 0.050\n",
      "svr_poly1 CV -> Pearson: 0.381 ± 0.184, RMSE: 6.484 ± 8.906\n",
      "XGB2 CV -> Pearson: 0.653 ± 0.028, RMSE: 1.476 ± 0.061\n",
      "Linear CV -> Pearson: 0.547 ± 0.142, RMSE: 2.028 ± 1.374\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.979, RMSE: 0.536\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.931, RMSE: 0.770\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.876, RMSE: 0.942\n",
      " TRAIN :: XGB2 Test -> Pearson: 1.000, RMSE: 0.036\n",
      " TRAIN :: Linear Test -> Pearson: 0.611, RMSE: 1.543\n",
      "TEST :: RandomForest9 Test -> Pearson: 0.730, RMSE: 1.415\n",
      "TEST :: GradientBoosting9 Test -> Pearson: 0.694, RMSE: 1.454\n",
      "TEST :: svr_poly1 Test -> Pearson: 0.442, RMSE: 2.677\n",
      "TEST :: XGB2 Test -> Pearson: 0.694, RMSE: 1.455\n",
      "TEST :: Linear Test -> Pearson: 0.653, RMSE: 1.532\n",
      "Model Number => 6\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 42\n",
      "RandomForest9 CV -> Pearson: 0.683 ± 0.026, RMSE: 1.426 ± 0.043\n",
      "GradientBoosting9 CV -> Pearson: 0.642 ± 0.023, RMSE: 1.497 ± 0.034\n",
      "svr_poly1 CV -> Pearson: 0.455 ± 0.123, RMSE: 2.421 ± 0.829\n",
      "XGB2 CV -> Pearson: 0.665 ± 0.024, RMSE: 1.455 ± 0.039\n",
      "Linear CV -> Pearson: 0.587 ± 0.025, RMSE: 1.578 ± 0.045\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.978, RMSE: 0.523\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.910, RMSE: 0.858\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.836, RMSE: 1.071\n",
      " TRAIN :: XGB2 Test -> Pearson: 0.998, RMSE: 0.133\n",
      " TRAIN :: Linear Test -> Pearson: 0.596, RMSE: 1.565\n",
      "TEST :: RandomForest9 Test -> Pearson: 0.732, RMSE: 1.397\n",
      "TEST :: GradientBoosting9 Test -> Pearson: 0.689, RMSE: 1.462\n",
      "TEST :: svr_poly1 Test -> Pearson: 0.627, RMSE: 1.698\n",
      "TEST :: XGB2 Test -> Pearson: 0.712, RMSE: 1.416\n",
      "TEST :: Linear Test -> Pearson: 0.662, RMSE: 1.526\n",
      "Model Number => 3\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Generate file names................................................................\n",
      "Begin to featurize dataset....................................................................\n",
      "Training samples: 4803, features: 64\n",
      "RandomForest9 CV -> Pearson: 0.682 ± 0.028, RMSE: 1.433 ± 0.051\n",
      "GradientBoosting9 CV -> Pearson: 0.650 ± 0.033, RMSE: 1.481 ± 0.056\n",
      "svr_poly1 CV -> Pearson: 0.350 ± 0.177, RMSE: 5.581 ± 6.164\n",
      "XGB2 CV -> Pearson: 0.664 ± 0.024, RMSE: 1.456 ± 0.042\n",
      "Linear CV -> Pearson: 0.579 ± 0.033, RMSE: 1.589 ± 0.051\n",
      "Test samples: 497\n",
      " TRAIN :: RandomForest9 Test -> Pearson: 0.979, RMSE: 0.526\n",
      " TRAIN :: GradientBoosting9 Test -> Pearson: 0.923, RMSE: 0.803\n",
      " TRAIN :: svr_poly1 Test -> Pearson: 0.847, RMSE: 1.039\n",
      " TRAIN :: XGB2 Test -> Pearson: 1.000, RMSE: 0.072\n",
      " TRAIN :: Linear Test -> Pearson: 0.594, RMSE: 1.569\n"
     ]
    }
   ],
   "source": [
    "sd = 50\n",
    "np.random.seed(sd)\n",
    "data_dir = \"/home/ansh-meshram/Desktop/work/bio_project/PDBbind\"\n",
    "spr_train = [0.9, 0.1, 0]\n",
    "spr_test = [1, 0, 0]\n",
    "trparadict = {'rf': {'rf_n_estimators': np.arange(300, 800, 100).tolist()},\n",
    "              'gb': {'gb_n_estimators': np.arange(300, 800, 100).tolist()},\n",
    "               'sv' : {}  \n",
    "            }\n",
    "datasets = ['rs-casf2016-csarhiq', 'casf2016', 'csarhiqS1', 'csarhiqS2', 'csarhiqS3']\n",
    "# 1.1. featurize datasets -----------------------------------------------------------------------------------\n",
    "#################################### IMCPs #######################################################################\n",
    "ifptp = 'rfscore_ext'\n",
    "cur_cutoff = 12\n",
    "bins = [[(0,12)],[(0,6),(6,12)],[(0,4),(4,8),(8,12)]]\n",
    "                      \n",
    "\n",
    "for cur_bins in bins:\n",
    "        para = {'addH': True, 'sant': False,\n",
    "        'cutoff': cur_cutoff, 'ifp_type': ifptp,\n",
    "        'bins': cur_bins}   \n",
    "        for modelno in range(3,7):\n",
    "                print(\"Model Number =>\",modelno)                \n",
    "                feat_dt_train = loaddata_fromPDBbind_v2020(data_dir = data_dir,modele = modelno,\n",
    "                                                        subset = datasets[0],\n",
    "                                                        select_target = None, \n",
    "                                                        randsplit_ratio = spr_train,\n",
    "                                                        para = para,\n",
    "                                                        rand_seed = sd)\n",
    "                feat_dt_test1 = loaddata_fromPDBbind_v2020(data_dir = data_dir,modele = modelno, \n",
    "                                                        subset = datasets[1], \n",
    "                                                        select_target = None, \n",
    "                                                        randsplit_ratio = spr_test,\n",
    "                                                        para = para,\n",
    "                                                        rand_seed = sd)\n",
    "\n",
    "                # 1.2 Model parameterization and validation -----------------------------------------------------------------\n",
    "                # results saved in 'res.txt' in the data folder ------------------------------------------------------------\n",
    "                group_val = 0\n",
    "                if modelno == 1:\n",
    "                        group_val = 2\n",
    "                elif modelno == 2:\n",
    "                        group_val = 1\n",
    "                elif modelno == 3:\n",
    "                        group_val = 2\n",
    "                elif modelno == 4:\n",
    "                        group_val = 3\n",
    "                elif modelno == 5:\n",
    "                        group_val = 4\n",
    "                else:\n",
    "                        group_val = 4\n",
    "                #round_robin_split_and_plot_correlation(feat_dt_train[0],feat_dt_train[1],group_val)\n",
    "                train_and_test(feat_dt_train[0],feat_dt_train[1],feat_dt_test1[0],feat_dt_test1[1],)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8857bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96ac79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
